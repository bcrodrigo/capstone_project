{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de4f0d1e-09b0-4b30-8787-7e0bae582303",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b696507c-9129-436b-aa18-806a850661c3",
   "metadata": {},
   "source": [
    "Notebook to perform EDA on a given dataset, as indicated in the title. In this case this is for the following dataset\n",
    "\n",
    "- `02_forest_fire_dataset` \n",
    "\n",
    "**Source:** https://www.kaggle.com/datasets/alik05/forest-fire-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda4cbc9-5c84-4e17-912b-49cd3609d036",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Directory Structure\n",
    "\n",
    "It is assumed that the directory structure for this dataset is organized as follows\n",
    "```bash\n",
    ".\n",
    "├── data_preprocessing/\n",
    "│   └── 02_forest_fire_dataset/\n",
    "│       ├── testing/\n",
    "│       │   ├── fire/\n",
    "│       │   └── nofire/\n",
    "│       └── training/\n",
    "│           ├── fire/\n",
    "│           └── nofire/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b191a-6caf-4d28-a6ab-828977d601d3",
   "metadata": {},
   "source": [
    "## Steps\n",
    "We will do the following for each of the `training/` and `testing/` directories\n",
    "1. Create a dataframe listing all images, labels, channels, width and height\n",
    "2. Review that all images are the correct number of channels (3) and have the same width and height (250 x 250)\n",
    "3. Crop or replace images as needed\n",
    "4. Update dataframe accordingly\n",
    "5. Save a .csv file with the annotation file to be used by PyTorch\n",
    "\n",
    "Note the final csv should be used by the dataloader on a separate notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1865f714-1452-4e47-9799-23bbfbb83985",
   "metadata": {},
   "source": [
    "# Define Helper Functions\n",
    "Note: in the future I want jupyter to import all helper functions contained in the `src/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316dfb77-d872-4b20-9a7e-2067e3dc1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from torchvision.io import read_image\n",
    "\n",
    "def review_dataset(path_to_dataset,dataset_foldername,img_folders):\n",
    "    '''\n",
    "    Function that creates the annotations file (.csv) for an image dataset.\n",
    "    The dataset is assumed to be divided into two folders according to the labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_dataset : str\n",
    "        String with the path pointing to the folder containing the dataset\n",
    "    dataset_foldername : str\n",
    "        Name of the folder containing all images\n",
    "    img_folders : list\n",
    "        List with the folder names containing the images. Both folders are assumed to be inside \n",
    "        `dataset_foldername` and each position in the list is going to be mapped to a binary category\n",
    "        img_folders[0] --> category 0\n",
    "        img_folders[1] --> category 1\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        A dataframe contaning the list of images and their binary label\n",
    "    \n",
    "    '''\n",
    "    assert isinstance(path_to_dataset,str),'path_to_dataset must be a string'\n",
    "    assert isinstance(dataset_foldername,str),'dataset_foldername must be a string'\n",
    "    assert isinstance(img_folders,list),'img_folders must be a list'\n",
    "    \n",
    "    Nfolders = len(img_folders)\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for index in range(Nfolders):\n",
    "    \n",
    "        curr_folder = img_folders[index]\n",
    "    \n",
    "        assert isinstance(curr_folder,str),'element of img_folders must be a string'\n",
    "    \n",
    "        curr_path = path_to_dataset + dataset_foldername + '/' + curr_folder\n",
    "    \n",
    "        image_list = os.listdir(curr_path)\n",
    "        labels = index * np.ones(len(image_list), dtype = int)\n",
    "    \n",
    "        temp_df = pd.DataFrame(columns = ['item'], data = image_list)\n",
    "    \n",
    "        # update each item to include the image folder \n",
    "        temp_df['item'] = temp_df['item'].apply(lambda x: curr_folder + '/' + x)\n",
    "    \n",
    "        # add column with labels\n",
    "        temp_df['label'] = labels\n",
    "    \n",
    "        df_list.append(temp_df.copy())\n",
    "    \n",
    "    # concatenate dataframe list into a single file\n",
    "    all_images_df = pd.concat(df_list, axis = 0, ignore_index = True)\n",
    "    \n",
    "    print('\\n Created dataframe with all images\\n')\n",
    "    print(all_images_df.info())\n",
    "    \n",
    "    original_dir = os.getcwd()\n",
    "    \n",
    "    os.chdir(path_to_dataset + dataset_foldername)\n",
    "    \n",
    "    # preallocate columns for image channels and size\n",
    "    all_images_df['channels'] = 0\n",
    "    all_images_df['height'] = 0\n",
    "    all_images_df['width'] = 0\n",
    "    \n",
    "    image_list = all_images_df['item'].to_list()\n",
    "    \n",
    "    for index,image in enumerate(image_list):\n",
    "    \n",
    "        img_shape = read_image(image).shape\n",
    "    \n",
    "        all_images_df.at[index,'channels'] = img_shape[0]\n",
    "        all_images_df.at[index,'height'] = img_shape[1]\n",
    "        all_images_df.at[index,'width'] = img_shape[2]\n",
    "\n",
    "    os.chdir(original_dir)\n",
    "    \n",
    "    return all_images_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "288c78bc-e359-4ba5-a7ba-6e0720fde845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import crop\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.io import read_image\n",
    "\n",
    "def crop_images(df_oversized,path_to_dataset,dataset_foldername,new_height,new_width):\n",
    "    '''\n",
    "    # helper function to crop images and save them\n",
    "    # inputs\n",
    "    # df with list of oversized images \n",
    "    # assumes it has the following columns: item, label, channels, height, width\n",
    "    # new image size\n",
    "    # path_to_dataset\n",
    "    # dataset_foldername   \n",
    "    \n",
    "    '''\n",
    "    assert isinstance(new_height,int) and isinstance(new_width,int), 'new dimensions should be integers'\n",
    "    \n",
    "    df_oversized['cropped_item'] = ''    \n",
    "    \n",
    "    original_dir = os.getcwd()\n",
    "    \n",
    "    os.chdir(path_to_dataset + dataset_foldername)\n",
    "    \n",
    "    for i in df_oversized.index:\n",
    "    \n",
    "        image_path_name = df_oversized.iat[i,0]\n",
    "    \n",
    "        extension = image_path_name[-4:]\n",
    "        image_name = image_path_name[0:-4].split('/')[-1]\n",
    "    \n",
    "        # rename image, including path\n",
    "        cropped_image_path_name = image_path_name[0:-4] + '_cropped' + extension\n",
    "        \n",
    "        # update dataframe\n",
    "        df_oversized.at[i,'cropped_item'] = cropped_image_path_name\n",
    "        \n",
    "        # read image\n",
    "        img = read_image(image_path_name)\n",
    "        \n",
    "        # crop image\n",
    "        temp = crop(img,0,0,new_height,new_width)\n",
    "        \n",
    "        # save image -- need to normalize to the 0 to 1 interval\n",
    "        save_image(temp/255,cropped_image_path_name)\n",
    "    \n",
    "    print(f'\\nCropped all images to {new_height} x {new_width}\\n')\n",
    "    \n",
    "    os.chdir(original_dir)\n",
    "    \n",
    "    return df_oversized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17897a-dccf-4ff3-b6d8-146ccd6226e4",
   "metadata": {},
   "source": [
    "# Define Paths to Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3deb21da-5eee-4849-849a-7a9c46b65084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rodrigo/Documents/BrainStation/Capstone Project/capstone_project/jupyter_notebooks\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c1aa5e4-2b80-4c14-aa5f-e64391a23bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_02_forest_fire_dataset.csv      \u001b[1m\u001b[36mtesting\u001b[m\u001b[m\n",
      "labels_02_forest_fire_dataset_prep.csv \u001b[1m\u001b[36mtraining\u001b[m\u001b[m\n",
      "temp.jpg\n"
     ]
    }
   ],
   "source": [
    "! ls ../data_preprocessing/02_forest_fire_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9ca7bd-983e-4adf-b860-cdc710c16b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths are relative to jupyter notebook location\n",
    "path_to_dataset = '../data_preprocessing/'\n",
    "dataset_foldername = '02_forest_fire_dataset'\n",
    "img_folders_training = ['training/nofire','training/fire']\n",
    "img_folders_testing = ['testing/nofire','testing/fire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea94df87-273b-434f-b44a-06d474c46a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Created dataframe with all images\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1632 entries, 0 to 1631\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   item    1632 non-null   object\n",
      " 1   label   1632 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 25.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_training = review_dataset(path_to_dataset,dataset_foldername,img_folders_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b533415-c885-4578-85d3-1ab60d497737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "      <th>channels</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training/nofire/nofire_0169.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training/nofire/nofire_0633.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training/nofire/nofire_0155.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training/nofire/nofire_0141.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training/nofire/nofire_0627.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              item  label  channels  height  width\n",
       "0  training/nofire/nofire_0169.jpg      0         3     250    250\n",
       "1  training/nofire/nofire_0633.jpg      0         3     250    250\n",
       "2  training/nofire/nofire_0155.jpg      0         3     250    250\n",
       "3  training/nofire/nofire_0141.jpg      0         3     250    250\n",
       "4  training/nofire/nofire_0627.jpg      0         3     250    250"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41e8c4-4d3f-49d2-8b7c-906a7e529fa2",
   "metadata": {},
   "source": [
    "We can see that the training set has 1576 images in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01ab49b7-1deb-47e5-99d8-6eb2bc18a691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Created dataframe with all images\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 380 entries, 0 to 379\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   item    380 non-null    object\n",
      " 1   label   380 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 6.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_testing = review_dataset(path_to_dataset,dataset_foldername,img_folders_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1efe9fa-80b6-4daa-bd8f-aef2def4f7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "      <th>channels</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>testing/nofire/nofire_0800.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>testing/nofire/nofire_0828.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>testing/nofire/nofire_0196.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing/nofire/nofire_0357.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>testing/nofire/nofire_0431.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             item  label  channels  height  width\n",
       "0  testing/nofire/nofire_0800.jpg      0         3     256    256\n",
       "1  testing/nofire/nofire_0828.jpg      0         3     250    250\n",
       "2  testing/nofire/nofire_0196.jpg      0         3     250    250\n",
       "3  testing/nofire/nofire_0357.jpg      0         3     250    250\n",
       "4  testing/nofire/nofire_0431.jpg      0         3     250    250"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588d3599-98cb-4dfd-afe6-a8337bd0923f",
   "metadata": {},
   "source": [
    "The testing dataset has 380 images. We see from the head that the images might be bigger than 250 x 250. We'll explore in detail in the next sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431078e7-abf3-4e82-a75d-2aa3b772ae98",
   "metadata": {},
   "source": [
    "# Checking image sizes for Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d14c01e7-12a0-456e-8ec3-79ca4795b411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "channels\n",
       "3    1632\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training['channels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c69c9-3861-4f99-a165-2070b484dd3d",
   "metadata": {},
   "source": [
    "All 1576 images in the training set have 3 channels. No surprises here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f979945-1bd6-4324-b3ac-871525f4c3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "height\n",
       "250    1576\n",
       "256      55\n",
       "252       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training['height'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f0db29a-7c01-466b-b777-71f3787e4053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "width\n",
       "250    1576\n",
       "256      55\n",
       "252       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training['width'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9fef74-e7b7-44f2-a392-221b0547c028",
   "metadata": {},
   "source": [
    "We see that in the dataset there are 56 images that do not have the same 250 x 250 size as all others (they are bigger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3041fe46-c558-4b8a-bcf6-411ec72b8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the oversized entries\n",
    "df_training_oversized = df_training.query('width > 250').copy()\n",
    "\n",
    "#re-start the indexing\n",
    "df_training_oversized.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4f0f2b8-6347-4f61-8a74-24c2c9b83fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "      <th>channels</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training/nofire/nofire_0790.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training/nofire/nofire_0801.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training/nofire/nofire_0815.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training/nofire/nofire_0793.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training/nofire/nofire_0792.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              item  label  channels  height  width\n",
       "0  training/nofire/nofire_0790.jpg      0         3     256    256\n",
       "1  training/nofire/nofire_0801.jpg      0         3     256    256\n",
       "2  training/nofire/nofire_0815.jpg      0         3     256    256\n",
       "3  training/nofire/nofire_0793.jpg      0         3     256    256\n",
       "4  training/nofire/nofire_0792.jpg      0         3     256    256"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_oversized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "242d3d0f-570f-4776-be23-2d6ea63110e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_oversized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711aa56-b1f8-4119-a54a-6b092e653dd4",
   "metadata": {},
   "source": [
    "There's exactly 56 images that are oversized on both dimensions. We'll add another column to store the re-sized name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebe9c91c-f748-404d-8dca-66c9d2f9612f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "      <th>channels</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training/nofire/nofire_0790.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training/nofire/nofire_0801.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training/nofire/nofire_0815.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training/nofire/nofire_0793.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training/nofire/nofire_0792.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              item  label  channels  height  width\n",
       "0  training/nofire/nofire_0790.jpg      0         3     256    256\n",
       "1  training/nofire/nofire_0801.jpg      0         3     256    256\n",
       "2  training/nofire/nofire_0815.jpg      0         3     256    256\n",
       "3  training/nofire/nofire_0793.jpg      0         3     256    256\n",
       "4  training/nofire/nofire_0792.jpg      0         3     256    256"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_oversized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5213cc07-d1c4-4486-ac39-3aa352dd388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs for image cropping\n",
    "new_height = 250\n",
    "new_width = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "728581de-df81-4baf-8f73-57efa4a9d8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cropped all images to 250 x 250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_training_oversized = crop_images(df_training_oversized,path_to_dataset,dataset_foldername,new_height,new_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8edb181-dda8-4f60-9303-af1b9a6c1f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "      <th>channels</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>cropped_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training/nofire/nofire_0790.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>training/nofire/nofire_0790_cropped.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training/nofire/nofire_0801.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>training/nofire/nofire_0801_cropped.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training/nofire/nofire_0815.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>training/nofire/nofire_0815_cropped.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training/nofire/nofire_0793.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>training/nofire/nofire_0793_cropped.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training/nofire/nofire_0792.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>training/nofire/nofire_0792_cropped.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              item  label  channels  height  width  \\\n",
       "0  training/nofire/nofire_0790.jpg      0         3     256    256   \n",
       "1  training/nofire/nofire_0801.jpg      0         3     256    256   \n",
       "2  training/nofire/nofire_0815.jpg      0         3     256    256   \n",
       "3  training/nofire/nofire_0793.jpg      0         3     256    256   \n",
       "4  training/nofire/nofire_0792.jpg      0         3     256    256   \n",
       "\n",
       "                              cropped_item  \n",
       "0  training/nofire/nofire_0790_cropped.jpg  \n",
       "1  training/nofire/nofire_0801_cropped.jpg  \n",
       "2  training/nofire/nofire_0815_cropped.jpg  \n",
       "3  training/nofire/nofire_0793_cropped.jpg  \n",
       "4  training/nofire/nofire_0792_cropped.jpg  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_oversized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15008db0-a735-4d76-9ec9-5323870b4250",
   "metadata": {},
   "source": [
    "Now we have to update the original dataframe with the cropped images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f43efd6-3b9d-4679-b7c4-54484fab94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the rows that have height (and width) of 250\n",
    "df_correct_size = df_training.query('height == 250').copy()\n",
    "df_correct_size.reset_index(drop = True, inplace = True)\n",
    "df_correct_size.drop(labels = ['channels','height','width'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0071a5c6-9ad2-4ecc-8359-c8d2f15bae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 'cropped_item' and 'label' columns from df_training_oversized\n",
    "df_updated_sizes = df_training_oversized[['cropped_item','label']].copy()\n",
    "df_updated_sizes.columns = ['item','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db4f7c53-b042-4f05-a10a-b4694bbc5d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenate dataframes\n",
    "updated_items_labels = pd.concat([df_correct_size,df_updated_sizes], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c68dea3-4f4c-46a6-b25d-1dbe488f6530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rodrigo/Documents/BrainStation/Capstone Project/capstone_project/jupyter_notebooks\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea7d1c37-fad6-45a1-acfa-b02bde5a5156",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data_preprocessing/'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e276b21-9fd3-4c33-9e08-c6886fbac7d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02_forest_fire_dataset'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_foldername"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5437a58f-70c1-4bae-84a7-5970937ff4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_path_new_annotations = path_to_dataset + dataset_foldername + '/' + 'labels_02_train_dataset_prep.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0273ca8c-1727-46eb-933d-72882c661c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_items_labels.to_csv(full_path_new_annotations,index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "259547fb-6690-421c-8ed9-83e93d5ef9cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_02_forest_fire_dataset.csv      temp.jpg\n",
      "labels_02_forest_fire_dataset_prep.csv \u001b[1m\u001b[36mtesting\u001b[m\u001b[m\n",
      "labels_02_train_dataset_prep.csv       \u001b[1m\u001b[36mtraining\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls ../data_preprocessing/02_forest_fire_dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c283c3f4-87d1-4877-82f0-6ecd92a58975",
   "metadata": {},
   "source": [
    "The labels file has been created for the training dataset. Now we move to the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c945ee-1b6e-46ba-a329-4c710bffe501",
   "metadata": {},
   "source": [
    "# Checking image sizes for Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96c85d35-4ea2-4ca0-8f63-5daa2010f97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "channels\n",
       "3    380\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing['channels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd238f-0c50-49c0-8535-b51a99720f02",
   "metadata": {},
   "source": [
    "No surprises on number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cacc993f-0bcf-40bc-89cf-aa8a523457c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "height\n",
       "250    362\n",
       "256     18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing['height'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8297a4ef-1b8e-40de-8713-ccca0a201a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "width\n",
       "250    362\n",
       "256     18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing['width'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e824f-335e-4f78-b405-45d3f732b13c",
   "metadata": {},
   "source": [
    "We see there are 18 images that are oversized. We'll follow a similar procedure as before to crop them down to 250 x 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e26d631-a7ed-42fa-b00b-c4691fcae4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the oversized entries\n",
    "df_testing_oversized = df_testing.query('width > 250').copy()\n",
    "\n",
    "#re-start the indexing\n",
    "df_testing_oversized.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "666d6395-14e4-42be-b942-4696f6c2cf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 5)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing_oversized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0edf681-97a6-4309-9427-54e73fd7c541",
   "metadata": {},
   "source": [
    "Only 18 entries are oversized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae3ca943-d3f3-4fb1-aeca-455fbd3252bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "      <th>channels</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>testing/nofire/nofire_0800.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>testing/nofire/nofire_0791.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>testing/nofire/nofire_0785.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing/nofire/nofire_0787.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>testing/nofire/nofire_0796.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             item  label  channels  height  width\n",
       "0  testing/nofire/nofire_0800.jpg      0         3     256    256\n",
       "1  testing/nofire/nofire_0791.jpg      0         3     256    256\n",
       "2  testing/nofire/nofire_0785.jpg      0         3     256    256\n",
       "3  testing/nofire/nofire_0787.jpg      0         3     256    256\n",
       "4  testing/nofire/nofire_0796.jpg      0         3     256    256"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing_oversized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09590929-50db-43e6-8e6c-ca8700d4cc2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cropped all images to 250 x 250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_height = 250\n",
    "new_width = 250\n",
    "df_testing_oversized = crop_images(df_testing_oversized,path_to_dataset,dataset_foldername,new_height,new_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d8ac22e-c260-487a-86c4-6349457ecbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "      <th>channels</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>cropped_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>testing/nofire/nofire_0800.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>testing/nofire/nofire_0800_cropped.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>testing/nofire/nofire_0791.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>testing/nofire/nofire_0791_cropped.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>testing/nofire/nofire_0785.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>testing/nofire/nofire_0785_cropped.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing/nofire/nofire_0787.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>testing/nofire/nofire_0787_cropped.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>testing/nofire/nofire_0796.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>testing/nofire/nofire_0796_cropped.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             item  label  channels  height  width  \\\n",
       "0  testing/nofire/nofire_0800.jpg      0         3     256    256   \n",
       "1  testing/nofire/nofire_0791.jpg      0         3     256    256   \n",
       "2  testing/nofire/nofire_0785.jpg      0         3     256    256   \n",
       "3  testing/nofire/nofire_0787.jpg      0         3     256    256   \n",
       "4  testing/nofire/nofire_0796.jpg      0         3     256    256   \n",
       "\n",
       "                             cropped_item  \n",
       "0  testing/nofire/nofire_0800_cropped.jpg  \n",
       "1  testing/nofire/nofire_0791_cropped.jpg  \n",
       "2  testing/nofire/nofire_0785_cropped.jpg  \n",
       "3  testing/nofire/nofire_0787_cropped.jpg  \n",
       "4  testing/nofire/nofire_0796_cropped.jpg  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing_oversized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b67c8b-e850-4dc8-9a83-04e3a920c824",
   "metadata": {},
   "source": [
    "Let's repeat the same steps as before, and update the records that have oversized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6df1ff8b-bde5-4a8c-8543-f1a5ffb0fc96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract only the rows that have height (and width) of 250\n",
    "df_correct_size = df_testing.query('height == 250').copy()\n",
    "df_correct_size.reset_index(drop = True, inplace = True)\n",
    "df_correct_size.drop(labels = ['channels','height','width'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c5d5762-669d-48bb-8069-671826f9fe88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract 'cropped_item' and 'label' columns from df_testing_oversized\n",
    "df_updated_sizes = df_testing_oversized[['cropped_item','label']].copy()\n",
    "df_updated_sizes.columns = ['item','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d21750f7-da40-4d68-9f29-1fc1d20a45b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenate dataframes\n",
    "updated_testing_labels = pd.concat([df_correct_size,df_updated_sizes], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58930807-83e7-4a5d-9786-121d10966749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_path_new_annotations = path_to_dataset + dataset_foldername + '/' + 'labels_02_test_dataset_prep.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13de63f8-93a1-413a-bd09-99a14cf2d306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_testing_labels.to_csv(full_path_new_annotations,index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a89bc17-2851-4667-953d-9483a8d95c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_02_forest_fire_dataset.csv      temp.jpg\n",
      "labels_02_forest_fire_dataset_prep.csv \u001b[1m\u001b[36mtesting\u001b[m\u001b[m\n",
      "labels_02_test_dataset_prep.csv        \u001b[1m\u001b[36mtraining\u001b[m\u001b[m\n",
      "labels_02_train_dataset_prep.csv\n"
     ]
    }
   ],
   "source": [
    "# to double check labels file was created\n",
    "! ls ../data_preprocessing/02_forest_fire_dataset/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env_kernel",
   "language": "python",
   "name": "pytorch_env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
